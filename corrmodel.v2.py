from bert_embedding import BertEmbedding
from scipy import spatial
import csv
from unidecode import unidecode
from scipy import stats
import numpy as np
import pandas as pd
from collections import defaultdict
import re

## This script makes use of an open source project to generate token embeddings with multilingual Bert
## The project was created by Gary Lai (https://gary-lai.com/) and is available in github:
## https://github.com/imgarylai/bert-embedding

# Some of the code is derived from the baseline algorithm provided for SemEval 2020, task 3
# See Carlos Santos Armendariz, Matthew Purver, Matej Ulcar et al. 2019. CoSimLex: 
#     A Resource for Evaluating Graded Word Similarity in Context arXiv:1912.05320v2

# It was modified by Li Tang (University of Zurich, UZH), as described in the paper
# "UZH at SemEval-2020 Task 3: Combining BERT with WordNet sense embeddings to predict 
# graded word similarity changes" (July 2020, submitted).

# Refquired input files, in the same directory as this Python code.
# - refresults.txt: output generated by the baseline algorithm
# - cosimlex_en.csv: gold labels
# - lmms_1024.bert-large-cased.txt: pre-calculated average vectors for word senses, from Loureiro et al. (2019)

# /data/data_en.tsv: word pairs and context data 

# load reference results from an earlier submission (without distortion correction model)
reffile = open('./refresults.txt', 'r')
refdata = reffile.read()
ref = refdata.split('\n')

labels = pd.read_csv('./cosimlex_en.csv', sep='\t') # labels and related code was added after the initial submission
 # of the paper, during paper revision based on reviewer comments. The labels were not available during post-evaluation
 # phase, when the distortion correction model was developed, with the hyperparameters listed in Table 1 of the paper,
 # and below in the 'param' variable. A slight improvement was possible when testing more hyperparameter combinations
 # together with the labels, as described in the paper. 

print("Loaded labels. Shape:", labels.shape)
print("\nHead:", labels.head())

param = ( # adjustable parameters described in Table 1, and Equation 1, in the paper, as w1, w2, w3, w4 (in this order)
    (3,5,5,0),
    (3,3.8,2.2,0.2),
    (3,3.8,2.2,0.9),
    (5,2.6,2.9,1.3),
    (5,2,4.5,1.5),
    (5,4,5,1.8),
    (5,3,6,2.5)
    )

print("Loading pretrained Wordnet sense embeddings (Bert 1024 cased) from Loureiro (2019), ")
# source of Loureiro pre-trained sense embeddings: https://github.com/danlou/lmms, LMMS 1024 (bert-large-cased)

loufile = open('./lmms_1024.bert-large-cased.txt', 'r')
loudata = loufile.read()
lou = loudata.split('\n')
print("Loaded.")
comp = lou[2].split(' ')
loulen = len(comp) - 1
print("Number of vectors:", len(lou))

lvec = defaultdict(lambda: [])
i = 0
for lrow in lou: # load Loureiro vectors
    i += 1
    c = lrow.split(' ')
    header = c[0]
    hit = re.search(r'(\w+)', header)
    vector = c[1:1025]
    try:
        hit = str(hit[0])
        if re.search(r'[-_]', hit):
            pass
        else:
            lvec[hit].append(list(map(float, vector))) # list of lists, as there can be several vectors for different senses of the same word!
    except:
        pass
    if i % 20000 == 0:
        print("- vectors loaded:", i)

print("vectors for different word senses now stored in lvec")
le = len(lvec)
print("- number of tokens with vectors:", le)



def unbolded(context):
    context = str.replace(context, '<strong>', '')
    return str.replace(context, '</strong>', '')

## Load bert model
bert_embedding = BertEmbedding(model='bert_24_1024_16', dataset_name='book_corpus_wiki_en_cased', max_seq_length=230)

def avgshift(targets, word1_context, word2_context):
    # select the most similar word sense vector (WSEsel)  
    # senseshiftlevel = how strong is the similarity of a context-dep vector with one target over another target?
    # results will then influence the distortion correction model

    targetcount = 1
    shifts = []
    for token in targets: # for target word 1 and target word 2
        maxsim1, maxsim2 = 0, 0
        if len(lvec[token]) > 0:
            vectoken = token
        else:
            vectoken = token[:-1]
            if len(lvec[vectoken]) == 0:
                break
        i = 0
        for vec in lvec[vectoken]:
            i += 1
            sim_sense1 = 1 - spatial.distance.cosine(word1_context, vec)
            sim_sense2 = 1 - spatial.distance.cosine(word2_context, vec)
            if sim_sense1 > maxsim1:
                maxsim1 = round(sim_sense1, 3)
            if sim_sense2 > maxsim2:
                maxsim2 = round(sim_sense2, 3)
        if targetcount < 2:
            simshift = round(maxsim1 - maxsim2, 3) # first token should see a higher value in maxsim1
        else:
            simshift = round(maxsim2 - maxsim1, 3) # second token should see a higher value in maxsim2
        shifts.append(simshift)
        targetcount += 1
    nshifts = len(shifts)
    try:
        avgshift = sum(shifts) / nshifts
    except:
        avgshift = ""
    return avgshift


languages = ('en')

runs = len(param)
print("Will do", runs, "runs. All hyper-parameters:\n", param)
runnum = 0
for currparam in param:
    fac1 = param[runnum][0]
    fac2a = param[runnum][1]
    fac2b = param[runnum][2]
    fac2c = param[runnum][3]
    prun = runnum + 1
    print("\nParameters for run num:", prun, "are:", fac1, fac2a, fac2b, fac2c)
    print("\nIterating through English word pairs, totally 340:")

    errors, rownum = 0, 0
    alllouval, rows, similarities, diffs, labelshifts = [], [], [], [], []

    with open(f'./data/data_en.tsv', 'r') as csvfile:
        csvreader = csv.DictReader(csvfile, delimiter='\t', quoting=csv.QUOTE_NONE)

        for index, row in enumerate(csvreader):

 
            print(f"{index} {row['word1']}-{row['word2']}")
            #print("\nContext 1: ", row['context1'])

            ## Results contains several results for several sentences
            results = bert_embedding([unbolded(row['context1'])])
            ## Result here contains the results for first sentence
            result = results[0]
            ## result[0] is a list with all the tokens
            ## result[1] is a list with the embeddings per token
            tokens = result[0]
            embeddings = result[1]
            
            i = 0
            for token, embedding in zip(tokens, embeddings):
                if unidecode(token.lower()) == unidecode(row['word1_context1'].lower()):
                    word1_context1 = embedding
                    i += 1
                    break

            for token, embedding in zip(tokens, embeddings):
                if unidecode(token.lower()) == unidecode(row['word2_context1'].lower()):
                    word2_context1 = embedding
                    i += 1
                    break
                    
            if i != 2:
                #print(f'i = {i}')
                #print(f'Word1_context1 = {unidecode(row["word1_context1"].lower())}')
                #print(f'Word2_context1 = {unidecode(row["word2_context1"].lower())}')
                errors += 1
      
            sim_context1 = 1 - spatial.distance.cosine(word1_context1, word2_context1)

            #print("BERT sim1:", sim_context1) 
            #print("Gold label:", labels['sim1'][rownum])

            # Selecting most similar word sense embedding from Loureiro et al. 2019 pre-calculated word sense vectors
            targets = [unidecode(row['word1_context1'].lower()), unidecode(row['word2_context1'].lower())]
            avgshift1 = avgshift(targets, word1_context1, word2_context1)


            ## Results contains several results for several sentences
            results = bert_embedding([unbolded(row['context2'])])
            ## Result here contains the results for first sentence
            result = results[0]
            ## result[0] is a list with all the tokens
            ## result[1] is a list with the embeddings per token
            tokens = result[0]
            embeddings = result[1]
            
            i = 0
            for token, embedding in zip(tokens, embeddings):
                if unidecode(token.lower()) == unidecode(row['word1_context2'].lower()):
                    word1_context2 = embedding
                    i += 1
                    break

            for token, embedding in zip(tokens, embeddings):
                if unidecode(token.lower()) == unidecode(row['word2_context2'].lower()):
                    word2_context2 = embedding
                    i += 1
                    break
            
            if i != 2:
                errors += 1

            sim_context2 = 1 - spatial.distance.cosine(word1_context2, word2_context2)

            #print("BERT sim2:", sim_context2) 
            #print("Gold label:", labels['sim2'][rownum])


            # Selecting most similar word sense embedding from Loureiro et al. 2019 pre-calculated word sense vectors
            targets = [unidecode(row['word1_context2'].lower()), unidecode(row['word2_context2'].lower())]
            avgshift2 = avgshift(targets, word1_context2, word2_context2)

            try:
                avgshift21 = round(avgshift2 - avgshift1, 3)
                #print("\n- sense similarity (Loureiro):", round(avgshift1, 3), round(avgshift2, 3), " in context1,2 -> shifts:", avgshift21)
            except:
                #print("\n- sense similarity (Loureiro):", avgshift1, avgshift2, " in context1,2")
                pass
            try:
                senseshiftlevel = (avgshift1 + avgshift2) / 2
            except:
                senseshiftlevel = 0



            similarities.append([sim_context1, sim_context2])
            dp = len(similarities)
            change = sim_context2 - sim_context1

            #print("BERT shift:", change)

            labelchange = labels['sim2'][rownum] - labels['sim1'][rownum]

            #print("Gold labels, shift:", labelchange)

            refval = round(float(ref[dp]), 3)

            #print("\n -> change:", round(change, 3), "   errors:", errors)
            finalval = round(refval + ((change - refval) / fac1), 3)
            if refval > 0:
                finalval = refval + ((abs(change - refval)) / fac1)
            else:
                finalval = refval - ((abs(change - refval)) / fac1)
            #print("\n- finalval:", finalval)
            #print(".    ref:", round(refval, 3), "  diff:", round(finalval - refval, 3))

            louval = fac2c + round((abs(senseshiftlevel) * fac2a + abs(avgshift21) * fac2b), 3)
            #print("\nLou val for distortion correction:", louval)

            finalval = round(finalval * louval, 3)
            #print("\nAfter Loureiro  adjustment:", finalval, "     ref:", refval)

            #print("Gold labels, shift:", labelchange)

            diffs.append(finalval)
            alllouval.append(louval)
            labelshifts.append(labelchange)
            rownum += 1

    # calculate correlation between diffs (predicted) and gold labels; this was added after the initial paper submission
    a = np.array(diffs)
    b = np.array(labelshifts)
    r = stats.pearsonr(a,b)
    r = round(abs(r[0]), 5)
    print("\n\n\n Pearson correlation:", r)

    resultline = str(runnum) + "," + str(fac1) + "," + str(fac2a) + "," + str(fac2b) + "," + str(fac2c) + "," + str(r) + "\n"
    print("Resultline was:", resultline)

    outfile = "./res1/results_subtask1." + str(runnum) + "_en.tsv"

    runnum += 1


    columns = ['diff']
    with open(outfile, 'w') as csvfile:
        csvwriter = csv.writer(csvfile, delimiter='\t', quoting=csv.QUOTE_NONE, quotechar='', escapechar='~')
        csvwriter.writerow(columns)
        for sim in diffs:
            csvwriter.writerow([sim])


